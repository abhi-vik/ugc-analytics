{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "import geograpy as gg\n",
    "import pycountry as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>tweet</th>\n",
       "      <th>timeStamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@BridportMan</td>\n",
       "      <td>Bridport</td>\n",
       "      <td>We'll be on Christmas break from December 16th...</td>\n",
       "      <td>5:54 AM - 27 Nov 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@HILeicesterCity</td>\n",
       "      <td>Leicester, England</td>\n",
       "      <td>Plan your pre-Christmas break to Leicester now...</td>\n",
       "      <td>8:08 AM - 27 Nov 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@afaranwide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunset over Phu Quoc, Vietnam.\\n\\nLook out for...</td>\n",
       "      <td>2:41 AM - 27 Nov 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Sarah___Mary</td>\n",
       "      <td>South Wales</td>\n",
       "      <td>When you realise you only have to survive one ...</td>\n",
       "      <td>11:21 PM - 26 Nov 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@EduClickAfrica</td>\n",
       "      <td>Centre, Cameroon</td>\n",
       "      <td>Our #Techlab offers affordable #excursion expe...</td>\n",
       "      <td>12:05 AM - 25 Nov 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user            location  \\\n",
       "0      @BridportMan            Bridport   \n",
       "1  @HILeicesterCity  Leicester, England   \n",
       "2       @afaranwide                 NaN   \n",
       "3     @Sarah___Mary         South Wales   \n",
       "4   @EduClickAfrica    Centre, Cameroon   \n",
       "\n",
       "                                               tweet               timeStamp  \n",
       "0  We'll be on Christmas break from December 16th...   5:54 AM - 27 Nov 2019  \n",
       "1  Plan your pre-Christmas break to Leicester now...   8:08 AM - 27 Nov 2019  \n",
       "2  Sunset over Phu Quoc, Vietnam.\\n\\nLook out for...   2:41 AM - 27 Nov 2019  \n",
       "3  When you realise you only have to survive one ...  11:21 PM - 26 Nov 2019  \n",
       "4  Our #Techlab offers affordable #excursion expe...  12:05 AM - 25 Nov 2019  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"output.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "df['location_filled'] = df['location'].fillna(\"\")\n",
    "df['Location_Token'] = df['location_filled'].apply(lambda x: word_tokenize(x))\n",
    "df['Tweet_Token'] = df['tweet'].apply(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country(list):\n",
    "    for country in pycountry.countries:\n",
    "        if country.name in list:\n",
    "            print(country.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aw', 'abw', 'aruba', 'af', 'afg', 'afghanistan', 'ao', 'ago', 'angola', 'ai']\n",
      "['canillo', 'encamp', 'la massana', 'ordino', 'sant julià de lòria', 'andorra la vella', 'escaldes-engordany', \"'ajmān\", 'abū ȥaby [abu dhabi]', 'dubayy']\n"
     ]
    }
   ],
   "source": [
    "countries_list = []\n",
    "for country in list(pc.countries):\n",
    "    countries_list.append(country.alpha_2.lower())\n",
    "    countries_list.append(country.alpha_3.lower())\n",
    "    countries_list.append(country.name.lower())\n",
    "    try:\n",
    "        countries_list.append(country.common_name.lower())\n",
    "    except:\n",
    "        None\n",
    "city_list = []\n",
    "for city in list(pc.subdivisions):\n",
    "    city_list.append(city.name.lower())\n",
    "print(countries_list[0:10])\n",
    "print(city_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_name(list):\n",
    "    if list == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        for value in list:\n",
    "            value = value.lower()\n",
    "            fuzz_score_country = {}\n",
    "            for country in countries_list:\n",
    "                fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "            for country in ['united states of america', 'murica', 'u.s.a.', 'usausausa','america', 'us of a','usa ',' usa', 'u s','u s a','u.s.','trumpistan','usa usa usa','pittsburgh','unhinged states','murrika','n. america','ahem....amerca','usa usa usa!!!!']:\n",
    "                fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "            for country in ['the netherlands','england','u.k.','uk','uk ',' uk','uae','korea','south korea']:\n",
    "                fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "            if max(fuzz_score_country, key=fuzz_score_country.get) in ['cascadia','united states of america', 'murica', 'u.s.a.', 'usausausa','america', 'us of a','usa ',' usa', 'u s','u s a','u.s.','trumpistan','usa usa usa','pittsburgh','unhinged states','murrika','n. america','ahem....amerca','usa usa usa!!!!']:\n",
    "                return pc.countries.lookup('united states').alpha_3\n",
    "            elif max(fuzz_score_country, key=fuzz_score_country.get) in ['england','u.k.','uk','uk ',' uk']:\n",
    "                return pc.countries.lookup('united kingdom').alpha_3\n",
    "            elif max(fuzz_score_country, key=fuzz_score_country.get) in ['uae']:\n",
    "                return pc.countries.lookup('United Arab Emirates').alpha_3\n",
    "            elif max(fuzz_score_country, key=fuzz_score_country.get) in ['korea','south korea']:\n",
    "                return pc.countries.lookup('KR').alpha_3\n",
    "            elif max(fuzz_score_country, key=fuzz_score_country.get) in ['the netherlands']:\n",
    "                return pc.countries.lookup('netherlands').alpha_3\n",
    "            elif fuzz_score_country[max(fuzz_score_country, key=fuzz_score_country.get)] <86:\n",
    "                fuzz_score_city = {}\n",
    "                for city in city_list:\n",
    "                    fuzz_score_city[city] = fuzz.ratio(value,city)\n",
    "                if fuzz_score_city[max(fuzz_score_city, key=fuzz_score_city.get)] <90:\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    return pc.countries.lookup(pc.subdivisions.lookup(max(fuzz_score_city, key=fuzz_score_city.get)).country_code).alpha_3\n",
    "            return pc.countries.lookup(max(fuzz_score_country, key=fuzz_score_country.get)).alpha_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for \n",
    "df['Location_Token'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/PY3/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/jass/nltk_data'\n    - '/Users/jass/anaconda3/nltk_data'\n    - '/Users/jass/anaconda3/share/nltk_data'\n    - '/Users/jass/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1e6f3ddb779e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgeograpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_place_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Location_Token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/geograpy/__init__.py\u001b[0m in \u001b[0;36mget_place_context\u001b[0;34m(url, text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_place_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/geograpy/extraction.py\u001b[0m in \u001b[0;36mfind_entities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/PY3/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/jass/nltk_data'\n    - '/Users/jass/anaconda3/nltk_data'\n    - '/Users/jass/anaconda3/share/nltk_data'\n    - '/Users/jass/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "geograpy.get_place_context(text = df['Location_Token'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in city_list:\n",
    "    print(city) if city == 'Bridport' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['Location_Classify'] = df_process['location'].map(lambda value: country_name(value))\n",
    "country_name(df['Location_Token'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "import pycountry as pc\n",
    "countries_list = []\n",
    "for country in list(pc.countries):\n",
    "    countries_list.append(country.alpha_2.lower())\n",
    "    countries_list.append(country.alpha_3.lower())\n",
    "    countries_list.append(country.name.lower())\n",
    "    try:\n",
    "        countries_list.append(country.common_name.lower())\n",
    "    except:\n",
    "        None\n",
    "city_list = []\n",
    "for city in list(pc.subdivisions):\n",
    "    city_list.append(city.name.lower())\n",
    "    \n",
    "def country_name(value):\n",
    "    if value is np.nan:\n",
    "        return np.nan\n",
    "    elif isinstance(value,int):\n",
    "        return np.nan\n",
    "    else:\n",
    "        value = value.lower()\n",
    "        fuzz_score_country = {}\n",
    "        for country in countries_list:\n",
    "            fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "        for country in ['united states of america', 'murica', 'u.s.a.', 'usausausa','america', 'us of a','usa ',' usa', 'u s','u s a','u.s.','trumpistan','usa usa usa','pittsburgh','unhinged states','murrika','n. america','ahem....amerca','usa usa usa!!!!']:\n",
    "            fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "        for country in ['the netherlands','england','u.k.','uk','uk ',' uk','uae','korea','south korea']:\n",
    "            fuzz_score_country[country] = fuzz.ratio(value,country)\n",
    "        if max(fuzz_score_country, key=fuzz_score_country.get) in ['cascadia','united states of america', 'murica', 'u.s.a.', 'usausausa','america', 'us of a','usa ',' usa', 'u s','u s a','u.s.','trumpistan','usa usa usa','pittsburgh','unhinged states','murrika','n. america','ahem....amerca','usa usa usa!!!!']:\n",
    "            return pc.countries.lookup('united states').alpha_3\n",
    "        elif max(fuzz_score_country, key=fuzz_score_country.get) in ['england','u.k.','uk','uk ',' uk']:\n",
    "            return pc.countries.lookup('united kingdom').alpha_3\n",
    "        elif max(fuzz_score_country, key=fuzz_score_country.get) in ['uae']:\n",
    "            return pc.countries.lookup('United Arab Emirates').alpha_3\n",
    "        elif max(fuzz_score_country, key=fuzz_score_country.get) in ['korea','south korea']:\n",
    "            return pc.countries.lookup('KR').alpha_3\n",
    "        elif max(fuzz_score_country, key=fuzz_score_country.get) in ['the netherlands']:\n",
    "            return pc.countries.lookup('netherlands').alpha_3\n",
    "        elif fuzz_score_country[max(fuzz_score_country, key=fuzz_score_country.get)] <86:\n",
    "            fuzz_score_city = {}\n",
    "            for city in city_list:\n",
    "                fuzz_score_city[city] = fuzz.ratio(value,city)\n",
    "            if fuzz_score_city[max(fuzz_score_city, key=fuzz_score_city.get)] <90:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return pc.countries.lookup(pc.subdivisions.lookup(max(fuzz_score_city, key=fuzz_score_city.get)).country_code).alpha_3\n",
    "        return pc.countries.lookup(max(fuzz_score_country, key=fuzz_score_country.get)).alpha_3\n",
    "\n",
    "# df_process['country'] = df_process['country'].fillna('')\n",
    "df_process['location'] = df_process['location'].map(lambda value: country_name(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "5      NaN\n",
       "6      NaN\n",
       "7      NaN\n",
       "8      GBR\n",
       "9      NaN\n",
       "10     NaN\n",
       "11     USA\n",
       "12     NaN\n",
       "13     NaN\n",
       "14     GBR\n",
       "15     NaN\n",
       "16     NaN\n",
       "17     NaN\n",
       "18     NaN\n",
       "19     NaN\n",
       "20     NaN\n",
       "21     GBR\n",
       "22     USA\n",
       "23     NaN\n",
       "24     NaN\n",
       "25     NaN\n",
       "26     NaN\n",
       "27     NaN\n",
       "28     NaN\n",
       "29     NaN\n",
       "      ... \n",
       "562    NaN\n",
       "563    NaN\n",
       "564    NaN\n",
       "565    NaN\n",
       "566    NaN\n",
       "567    NaN\n",
       "568    USA\n",
       "569    NaN\n",
       "570    NaN\n",
       "571    NaN\n",
       "572    NaN\n",
       "573    NaN\n",
       "574    NaN\n",
       "575    NaN\n",
       "576    NaN\n",
       "577    NaN\n",
       "578    NaN\n",
       "579    NaN\n",
       "580    NaN\n",
       "581    NaN\n",
       "582    NaN\n",
       "583    USA\n",
       "584    NaN\n",
       "585    NaN\n",
       "586    NaN\n",
       "587    NaN\n",
       "588    NaN\n",
       "589    NaN\n",
       "590    NaN\n",
       "591    NaN\n",
       "Name: location, Length: 592, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_process['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
