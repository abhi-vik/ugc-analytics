{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_string_to_list(series):\n",
    "    return (series.fillna('[]')\n",
    "            .map(lambda v: v.replace('\\'', '\\\"'))\n",
    "            .map(lambda v: json.loads(v)))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "df = pd.read_csv('data/output_combined_30k_locations_plus.csv', index_col='id')\n",
    "df['countries'] = series_string_to_list(df['countries'])\n",
    "df['cities'] = series_string_to_list(df['cities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_city_to_country_substitutions = {\n",
    "    'Indonesia': ['Bali'],\n",
    "    'England': ['London', 'Oxford', 'Liverpool', 'Manchester', 'Southampton',\n",
    "               'Plymouth'],\n",
    "    'United States of America': ['York', 'Florida', 'Miami', 'Chicago', 'Denver', \n",
    "                                 'Colorado', 'Atlanta', 'Seattle', 'Washington'],\n",
    "    'Mexico': ['Mexico'],\n",
    "    'France': ['Paris'],\n",
    "    'Canada': ['Toronto', 'Ontario'],\n",
    "    'Italy': ['Venice', 'Florence'],\n",
    "    'Ireland': ['Dublin'],\n",
    "    'Austria': ['Vienna'],\n",
    "    'India': ['Manali', 'Goa', 'Mumbai', 'Delhi', 'Shimla', 'Udaipur'],\n",
    "    'Spain': ['Barcelona'],\n",
    "    'Germany': ['Berlin'],\n",
    "    'Netherlands': ['Amsterdam'],\n",
    "    'Portugal': ['Lisbon'],\n",
    "    'Japan': ['Tokyo'],\n",
    "    'Czechia': ['Prague'],\n",
    "    'United Arab Emirates': ['Dubai', 'Ajman'],\n",
    "    'Ireland': ['Ireland']\n",
    "}\n",
    "\n",
    "inv_custom_city_to_country_substitutions = {word: root for root, words in \n",
    "                                            custom_city_to_country_substitutions.items() for word in words}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for city in row['cities']:\n",
    "        if city in inv_custom_city_to_country_substitutions.keys():\n",
    "            row['countries'] += [inv_custom_city_to_country_substitutions[city]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_words = {'com', 'twitter', 'travel', 'vacation', 'holiday', \n",
    "              'destination', 'flight', 'deal', '00', 'u', \n",
    "              'english', 'bali', 'weekend', 'thanksgiving', 'christmas', \n",
    "              'trip', 'best', 'new', 'news', 'london', \n",
    "              'day', 'sale', 'traveller', 'book'}\n",
    "\n",
    "same_map = {\n",
    "    'photo': ['pic', 'travelblogger', 'blog', 'photography', 'travelgram', \n",
    "              'travelphotography', 'beautiful', 'photographytour'],\n",
    "    'private': ['getaway', 'escape'],\n",
    "    'nature': ['experience', 'adventure', 'explore', 'outdoor', 'skiing',\n",
    "              'beautiful', 'paradise', 'island', 'stunning', 'waterfall', \n",
    "               'scenic', 'garden'],\n",
    "    'beach': ['cruise', 'island', 'sunset', 'beachvacation'],\n",
    "    'luxury': ['luxurytravel', 'resort', 'luxurypic', 'luxurious', 'spa', 'hospitality'],\n",
    "    'hotel': ['hoteldeals'],\n",
    "    'tour': ['guide'],\n",
    "    'family': ['disney', 'familytimepic', 'familytrip', 'familypic', 'familytime', 'familytravel'],\n",
    "    'romantic' : ['forher', 'forhim', 'engagement', 'lovepic', 'weddingplanner', \n",
    "                  'weddinginspiration', 'weddingstationery', 'lovetravel', 'lover', 'romance', \n",
    "                  'dateideashttps', 'dateideas', 'dateideashttp', 'wedding']\n",
    "}\n",
    "\n",
    "same_inv_map = {word: root for root, words in same_map.items() for word in words}\n",
    "\n",
    "def splitter(data):\n",
    "    words = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', data.lower()).split()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def convert_same(text):\n",
    "    return [(same_inv_map[word] if word in same_inv_map else word) for word in splitter(text)]\n",
    "\n",
    "def wordCount(data):\n",
    "    filtered = [w for w in convert_same(' '.join(data['text'])) if w not in stop_words|drop_words]\n",
    "    return Counter(filtered).most_common()\n",
    "\n",
    "# wordCount(df)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].map(convert_same)\n",
    "\n",
    "missed_countries = {\n",
    "    'Scotland': ['scotland'],\n",
    "    'Czechia': ['czech', 'czechia'],\n",
    "    'Wales': ['wales'],\n",
    "    'England': ['england'],\n",
    "    'United States of America': ['usa']\n",
    "}\n",
    "\n",
    "inv_missed_countries = {word: root for root, words in missed_countries.items() for word in words}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for city in row['tokens']:\n",
    "        if city in inv_missed_countries.keys():\n",
    "            row['countries'] += [inv_missed_countries[city]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countries\n",
       "England                     4702.0\n",
       "Scotland                    2259.0\n",
       "United States of America    1233.0\n",
       "India                        705.0\n",
       "Indonesia                    538.0\n",
       "China                        399.0\n",
       "Italy                        334.0\n",
       "Canada                       328.0\n",
       "Spain                        278.0\n",
       "France                       271.0\n",
       "Name: vectors, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(set(df['countries'].sum()), columns=['countries']).set_index('countries')\n",
    "data['tweets'] = 'temp-value'\n",
    "data['tweets'] = data['tweets'].map(lambda _: set())\n",
    "\n",
    "for tweet_id, row in df.iterrows():\n",
    "    for country in row['countries']:\n",
    "        data.loc[country]['tweets'].add(tweet_id)\n",
    "\n",
    "feature_words = ['photo', 'private', 'city', 'nature', 'beach', 'tour', 'family', 'romantic', 'hotel', 'luxury']\n",
    "\n",
    "feature_pos_di = dict(zip(feature_words, list(range(len(feature_words)))))\n",
    "\n",
    "def vectorize_tokens(tokens):\n",
    "    vector = np.zeros(len(feature_words))\n",
    "    for index, feature_word in enumerate(feature_words):\n",
    "        for token in tokens:\n",
    "            if token == feature_word:\n",
    "                vector[index] += 1\n",
    "    return vector\n",
    "\n",
    "def aggregate(tweets): return sum([tweet_ids_to_tokens[tweet] for tweet in tweets])\n",
    "\n",
    "tweet_ids_to_tokens = df['tokens'].map(vectorize_tokens).to_dict()\n",
    "\n",
    "data['vectors'] = data['tweets'].map(aggregate)\n",
    "\n",
    "data['vectors'].map(sum).nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(series, query):\n",
    "    query_vector = series.loc[query]\n",
    "    reduced_series = series.drop(['China', 'India', 'United States of America', 'England'])\n",
    "    matrix = pd.DataFrame(reduced_series.tolist()).values\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1+5, algorithm='auto', metric='cosine').fit(matrix)\n",
    "    distances, similar_items = nn.kneighbors([query_vector])\n",
    "\n",
    "    similar_item_names = reduced_series.iloc[similar_items[0]].index.tolist()\n",
    "    \n",
    "    return similar_item_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = pd.DataFrame([recommend(data['vectors'], country) for country in data.index], index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countries</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mauritius</th>\n",
       "      <td>Seychelles</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cambodia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>Bolivia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mozambique</td>\n",
       "      <td>Madagascar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Haiti</th>\n",
       "      <td>Palau</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Malta</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>Macao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Myanmar</td>\n",
       "      <td>Seychelles</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Cambodia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0        1           2           3            4\n",
       "countries                                                          \n",
       "Mauritius  Seychelles   Cyprus    Barbados      Greece     Cambodia\n",
       "Italy         Georgia  Bolivia   Australia  Mozambique   Madagascar\n",
       "Haiti           Palau   Brazil       Malta   Greenland        Macao\n",
       "Malaysia     Thailand  Myanmar  Seychelles      Israel     Cambodia\n",
       "Spain      Luxembourg  Denmark        Cuba      Norway  Netherlands"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations.loc[['Mauritius', 'Italy', 'Haiti', 'Malaysia', 'Spain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.to_csv('data/recommendations.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
